version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      RAG_EMBEDDING_MODEL: ${RAG_EMBEDDING_MODEL}
      RAG_CHAT_MODEL: ${RAG_CHAT_MODEL}
      HUGGING_FACE_API_KEY: ${HUGGING_FACE_API_KEY}
      OLLAMA_HOST: ${OLLAMA_HOST}
      WEAVIATE_HOST: ${WEAVIATE_HOST}
    networks:
      - tunedd-network
    depends_on:
      - ollama
      - weaviate

  ollama:
    image: ollama/ollama:latest

    entrypoint: >
      /bin/bash -c "
      ollama serve & 
      sleep 5 &&
      ollama pull nomic-embed-text &&
      echo 'Waiting for Ollama to serve...' &&
      until curl -s http://localhost:11434 | grep -q 'Ollama is running'; do
        sleep 2;
      done &&
      echo 'Ollama is ready!' &&
      wait"

    environment:
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      - "11435:11434"
    volumes:
      - tunedd-ollama-data:/root/.ollama
    networks:
      - tunedd-network
    restart: on-failure


  weaviate:
    image: semitechnologies/weaviate:1.25.1
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate/data
      - QUERY_DEFAULTS_LIMIT=20
      - CLUSTER_HOSTNAME=node1
      - GRPC_ENABLED=true
    volumes:
      - tunedd-weaviate-data:/var/lib/weaviate/data
    networks:
      - tunedd-network
    restart: on-failure

networks:
  tunedd-network:
    driver: bridge

volumes:
  tunedd-weaviate-data:
  tunedd-ollama-data:
